{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie języka naturalnego w Pythonie\n",
    "\n",
    "Ten notatnik ma na celu przedstawienie podstawowych kroków, które należy wykonać podczas analizy i klasyfikacji tekstów. W trakcie zadania będziemy tworzyć szereg funkcji pomocniczych, które na koniec wykorzystamy do klasyfikacji krótkich dokumentów tekstowych.\n",
    "\n",
    "Po wykonaniu tego zadania powinieneś:\n",
    "+ wiedzieć na czym polega i jak wykonać tokenizację,\n",
    "+ potrafić wykonać podstawowy stemming,\n",
    "+ potrafić analizować najpopularniejsze i narzadsze tokeny w zbiorze dokumentów,\n",
    "+ zdefiniować i usunąć stopwords,\n",
    "+ wiedzieć jak zamienić zbiór dokumentów na reprezentację bag-of-words,\n",
    "+ uruchomić wybrany klasyfikator na przetworzonym zbiorze danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie\n",
    "\n",
    "Na początek trochę bibliotek i przydatnych wyrażeń regularnych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka [re](https://docs.python.org/2/library/re.html) pozwala definiować wyrażenia regularne, [pandas](http://pandas.pydata.org/) już znasz z ostatnich zajęć, a [nltk](http://www.nltk.org/) to podstawowa biblioteka do przetwarzania języka naturalnego w pythonie. `nltk` to spory zestaw modułów, który domyślnie nie jest instalowany w całości. Aby doinstalować wybrane moduły możesz w interaktywnej konsoli pythona wpisać:\n",
    "\n",
    "`import nltk\n",
    "nltk.download()`\n",
    "\n",
    "Następnie w odpowiednich zakładkach wybrać interesujące moduły. W ramach tego ćwiczenia będziemy wykorzystywać moduł **`punkt`** z zakładki Modules, ale proszę zwrócić uwagę na interesujące korpusy i leksykony, takie jak np. `opinion_lexicon` czy `sentiwordnet`.\n",
    "\n",
    "Przyda nam się też zbiór danych do testowania kodu.\n",
    "\n",
    "**Zad. 1: Wczytaj zbiór danych `tweets_train.tsv` do zmiennej `tweets`. Pomiń nagłówek i pozostaw tylko ostatnią kolumnę. Wyświetl pierwsze 6 wierszy, żeby upewnić się, że każdy przykład składa się tylko z tekstu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja i stemming\n",
    "\n",
    "Żeby uporządkować trochę kod i kolejne kroki, wprowadźmy klasę Tokenizer i BeforeTokenizationNormalizer. Pierwsza będzie służyć jako klasa bazowa dla tworzonych przez tokenizatorów a druga zamieni encje html na poprawne znaki tekstowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        pass\n",
    "    \n",
    "class BeforeTokenizationNormalizer():\n",
    "    @staticmethod\n",
    "    def normalize(text):\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('&nbsp;', ' ')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&pound;', u'£')\n",
    "        text = text.replace('&euro;', u'€')\n",
    "        text = text.replace('&copy;', u'©')\n",
    "        text = text.replace('&reg;', u'®')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby sprawdzić czy wszystko działa - odkomentuj poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in tweets.index:\n",
    "#     tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W kolejnych krokach będziesz implementował coraz bardziej wyrafinowane tokenizatory. Zacznijmy od czegoś w miarę intuicyjnego.\n",
    "\n",
    "**Zad. 2: Napisz tokenizator, który dzieli słowa według spacji. Tokenizator powinien przyjmować tekst (pojedynczego stringa) i zamieniać go na listę słów. Przetestuj kod za pomocą zakomentowanej pętli.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer(Tokenizer):\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        # Można w tym celu wykorzystać wyrażenie RE_SPACES. Jakie wady ma takie podejście? Jak reaguje na znaki\n",
    "        # interpunkcyjne? Jeśli usuniesz interpunkcje, co się dzieje ze skrótowcami? Jak obsłużyć słowa can't, o'clock,\n",
    "        # we'd itp.?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in tweets.index:\n",
    "#     tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "#     words_simple = SimpleTokenizer.tokenize(tweet)\n",
    "#     print(words_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 3: Napisz tokenizator korzystający z funkcji word_tokenize() z biblioteki NLTK.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NltkTokenizer(Tokenizer):\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        # Napisz tokenizator korzystający z funkcji word_tokenize() z biblioteki NLTK.\n",
    "        # Czy w przypadku tweetów wszystkie słowa zostały poprawnie rozdzielone?\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in tweets.index:\n",
    "#     tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "#     words_nltk = NltkTokenizer.tokenize(tweet)\n",
    "#     print(words_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 4: Napisz tokenizator, który oprócz standardowych słów obsłuży emitikony i hashtagi. Następnie wykonaj stemming (porter = nltk.PorterStemmer() porter.stem()).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TweetTokenizer(Tokenizer):\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = SimpleTokenizer.tokenize(text)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            match = None\n",
    "            # sprawdź czy w ramach tokena występuje emotikona, hashtag lub link\n",
    "            if match is not None:\n",
    "                pass\n",
    "                # wydziel emotikonę lub hashtag jako token a resztę tekstu rozpatrz ponownie\n",
    "            else:\n",
    "                del tokens[i]\n",
    "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
    "            i += 1\n",
    "            \n",
    "        # stwórz stemmer i w pętli stemmuj wszystkie tokeny\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in tweets.index:\n",
    "#     tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "#     words_tweet = TweetTokenizer.tokenize(tweet)\n",
    "#     print(words_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W wielu zastosowaniach dobrze działa lematyzacja, która również jest dostępna w `nltk`. W tym ćwiczeniu pozostaniemy jednak przy stemmingu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tej części przeanalizujemy częstość występowania różnych słów w korpusie. Interesują nas słowa występujące najczęściej i najrzadziej. Ich analiza pozwoli określić jakie słowa powinny zostać pominięte podczas analizy a jakie powinny zostać połączone w grupy znaczeniowe.\n",
    "\n",
    "W tym celu przyda nam się jakaś struktura danych do zliczania słów oraz funkcja do rysowania histogramów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "def show_histogram(word_counts, title=None):\n",
    "    plot_df = pd.DataFrame.from_dict(word_counts).rename(columns={0:'Token', 1:'Count'})\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(12, 15))\n",
    "    p = sns.barplot(x=\"Count\", y=\"Token\", data=plot_df, color=\"b\")\n",
    "    p.set(xlabel=\"Count\", ylabel=\"\", title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy od zliczenia unikatowych słów. Pomoże nam w tym klasa [Counter](https://docs.python.org/2/library/collections.html#collections.Counter). Poniższy kod wypełnia kolekcję słowami po tokenizacji i stemmingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "\n",
    "# for i in tweets.index:\n",
    "#     tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "#     words.update(TweetTokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 5: Wypisz, zwizualizuj i przeanalizuj 50 najczęściej występujących słów. Czy wszystkie wyświetlone tokeny będą przydatne w dalszej analizie? Czy wszystkie tokeny to słowa?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Widać, że nie wszystkie tokeny to słowa. Ponieważ budujemy słownik słów, które będą wykorzystywane do klasyfikacji tekstów, usuńmy podstawowe tokeny, które nie niosą żadnej informacji.\n",
    "\n",
    "**Zad. 6: Usuń znaki interpunkcyjne z kolekcji words i powtórz analizę. Czy w przypadku badania opinii trzeba wyrzucić wszystkie znaki interpunkcyjne?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyraźnie widać, że nawet bez znaków intepunkcyjnych pozostało dużo zbędnych tokenów. Poniżej lista słów, które często są oznaczane jako tzw. stopwords, czyli słowa występujące często a nie niosące informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
    "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
    "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
    "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
    "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
    "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
    "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
    "            \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 7: Korzystając z listy `sotpwords` usuń z kolekcji `words` popularne stopwords i ponów analizę. Czy wszystkie stopwords zawsze należy wyrzucać? Czy słowa takie jak \"not\" mogą być nośnikiem opinii? Jeśli masz jeszcze dużo czasu, możesz przeanalizować listę słów w nltk.corpus.stopwords.words('english')?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tokenów, które występują zbyt często, problemem bywają również najrzadsze tokeny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 8: Przeanalizuj 100 listę najrzadszych tokenów. Czy wszystkie tokeny są rzeczywiście rzadkie? Czy jakieś typy tokenów powtarzają się? Jak myślisz, co najlepiej zrobić z takimi tokenami jak 1, 2, 3... 10%, 20%, 30%..., 12:00, 19:50, 22:30, ..., 2005, 2010, 1995..., 10\\$, 20\\$, 30\\$... ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na tym etapie w zmiennej `words` mamy wstępnie przygotowany słownik. Jak widać słownik możnaby jeszcze ulepszyć, ale w tym ćwiczeniu poprzestaniemy na tym co do tej pory zrobiliśmy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając słownik zamienimy przykłady uczące na reprezentację bag of words (BOW). W tej reprezentacji każdy przykład uczący (pojedynczy dokument tekstowy) przyjmuje formę wektora liczb. Każda pozycja w wektorze oznacza kolejne słowo ze słownika, a liczba na danej pozycji mówi czy (0-1) lub jak często (0-k) dane słowo występuje w tekście. Taka reprezentacja wektorowa ma tę zaletę, że można ją łatwo rozszerzać o dodatkowe atrybuty.\n",
    "\n",
    "**Zad. 9: Przeanalizuj poniższy kod tworzący reprezentację bag-of-words. Co to jest `csr_matrix`? Czy wykorzystanie tej klasy jest konieczne?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bow(documents, features):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in documents.index:\n",
    "        tweet = BeforeTokenizationNormalizer.normalize(documents.iloc[i, 2])\n",
    "        label = documents.iloc[i, 1]\n",
    "        tweet_tokens = TweetTokenizer.tokenize(tweet)\n",
    "\n",
    "        labels.append(label)\n",
    "        for token in set(tweet_tokens):\n",
    "            if token not in features:\n",
    "                continue\n",
    "            row.append(i)\n",
    "            col.append(features[token])\n",
    "            data.append(1)\n",
    "    return csr_matrix((data, (row, col)), shape=(len(documents), len(features))), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec wykorzystajmy dotychczasową wiedzę, żeby nauczyć i przetestować klasyfikator.\n",
    "\n",
    "**Zad. 10: Sprawdź działanie klasyfikatora Random Forest na przetworzonych wcześniej danych. Jak na trafność klasyfikacji wpływa parametr min_word_count?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Testing...\n",
      "=================== Results ===================\n",
      "            Positive    Neutral     Negative   \n",
      "F1        [ 0.18518519  0.17073171  0.66423358]\n",
      "Precision [ 0.12820513  0.41176471  0.61073826]\n",
      "Recall    [ 0.33333333  0.10769231  0.728     ]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "min_word_count = 5\n",
    "\n",
    "train_tweets = pd.read_csv(\"tweets_train.tsv\", sep=\"\\t\", header=None)\n",
    "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
    "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
    "\n",
    "feature_dict = {}\n",
    "for word in common_words:\n",
    "    feature_dict[word] = len(feature_dict)\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
    "list_of_labels = list(set(y_train))\n",
    "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Testing...\")\n",
    "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
    "X_test, y_test = create_bow(test_tweets, feature_dict)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"=================== Results ===================\")\n",
    "print(\"            Positive    Neutral     Negative   \")\n",
    "print(\"F1       \", f1_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
    "print(\"Precision\", precision_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
    "print(\"Recall   \", recall_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
